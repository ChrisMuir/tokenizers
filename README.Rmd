---
output:
  md_document:
    variant: markdown_github
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
```

## tokenizers

An R package that collects functions with a consistent interface to convert natural language text into tokens.

**Author:** [Lincoln Mullen](http://lincolnmullen.com)<br>
**License:** [MIT](http://opensource.org/licenses/MIT)<br>

[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/tokenizers)](http://cran.r-project.org/package=tokenizers)
[![Travis-CI Build Status](https://travis-ci.org/lmullen/tokenizers.svg?branch=master)](https://travis-ci.org/lmullen/tokenizers)

### Installation

To get the development version from GitHub, use  [devtools](https://github.com/hadley/devtools).

```{r eval=FALSE}
# install.packages("devtools")
devtools::install_github("lmullen/tokenizers")
```

### Examples

The tokenizers in this package have a consistent interface. They all take either a character vector of any length, or a list where each element is a character vector of length one. The idea is that each element comprises a text. Then each function returns a list with the same length as the input vector, where each element in the list are the tokens generated by the function. If the input character vector or list is named, then the names are preserved, so that the names can serve as identifiers.

```{r}
library(tokenizers)
james <- paste0(
  "The question thus becomes a verbal one\n",
  "again; and our knowledge of all these early stages of thought and feeling\n",
  "is in any case so conjectural and imperfect that farther discussion would\n",
  "not be worth while.\n",
  "\n",
  "Religion, therefore, as I now ask you arbitrarily to take it, shall mean\n",
  "for us _the feelings, acts, and experiences of individual men in their\n",
  "solitude, so far as they apprehend themselves to stand in relation to\n",
  "whatever they may consider the divine_. Since the relation may be either\n",
  "moral, physical, or ritual, it is evident that out of religion in the\n",
  "sense in which we take it, theologies, philosophies, and ecclesiastical\n",
  "organizations may secondarily grow.\n"
)

tokenize_characters(james)[[1]] %>% head(50)
tokenize_words(james)[[1]] %>% head(10)
tokenize_word_stems(james)[[1]] %>% head(10)
tokenize_sentences(james) 
tokenize_paragraphs(james)
tokenize_ngrams(james, n = 5, n_min = 2)[[1]] %>% head(10)
tokenize_skip_ngrams(james, n = 5, k = 2)[[1]] %>% head(10)
tokenize_lines(james)[[1]] %>% head(5)
tokenize_regex(james, pattern = "[,.;]")[[1]] %>% head(5)
```

### Contributing

Contributions to the package are more than welcome. One way that you can help is by using this package in your R package for natural language processing. If you want to contribute a tokenization function to this package, it should follow the same conventions as the rest of the functions whenever it makes sense to do so. 

Please note that this project is released with a [Contributor Code of Conduct](CONDUCT.md). By participating in this project you agree to abide by its terms.
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/basic-tokenizers.R
\name{basic-tokenizers}
\alias{basic-tokenizers}
\alias{tokenize_chars}
\alias{tokenize_lines}
\alias{tokenize_paragraphs}
\alias{tokenize_sentences}
\alias{tokenize_words}
\title{Basic tokenizers}
\usage{
tokenize_chars(x, lowercase = TRUE, strip_non_alphanum = TRUE)

tokenize_words(x, lowercase = TRUE)

tokenize_sentences(x, lowercase = FALSE, strip_punctuation = FALSE)

tokenize_lines(x)

tokenize_paragraphs(x, paragraph_break = "\\n\\n")
}
\arguments{
\item{x}{A character vector or list of character vectors to be tokenized.}

\item{lowercase}{Should the tokens be made lower case? The default value
varies by tokenizer; it is only \code{TRUE} by default for the tokenizers
that you are likely to use last.}

\item{strip_non_alphanum}{Should punctuation and white space be stripped?}

\item{strip_punctuation}{Should punctuation be stripped?}

\item{paragraph_break}{A string identifying the boundary between two
paragraphs.}
}
\value{
A character vector or list of character vectors containing the
  tokens.
}
\description{
These functions perform basic tokenization into words, sentences, paragraphs,
lines, and characters. The functions can be piped into one another to create
at most two levels of tokenization. For instance, one might split a text into
paragraphs and then word tokens, or into sentences and then word tokens.
}
\examples{
song <-  paste0("How many roads must a man walk down\\n",
                "Before you call him a man?\\n",
                "How many seas must a white dove sail\\n",
                "Before she sleeps in the sand?\\n",
                "\\n",
                "How many times must the cannonballs fly\\n",
                "Before they're forever banned?\\n",
                "The answer, my friend, is blowin' in the wind.\\n",
                "The answer is blowin' in the wind.\\n")

tokenize_words(song)
tokenize_sentences(song)
tokenize_paragraphs(song)
tokenize_lines(song)
tokenize_chars(song)

song \%>\%
  tokenize_paragraphs() \%>\%
  tokenize_sentences()

song \%>\%
  tokenize_sentences() \%>\%
  tokenize_words()
}


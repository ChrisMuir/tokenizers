Package: tokenizers
Type: Package
Title: A Consistent Interface to Tokenize Natural Language Text
Version: 0.1.4.9001
Description: Convert natural language text into tokens. The tokenizers have a
    consistent interface and are compatible with Unicode, thanks to being built
    on the 'stringi' package. Includes tokenizers for shingled n-grams, skip
    n-grams, words, word stems, sentences, paragraphs, characters, shingled 
    characters, lines, tweets, Penn Treebank, regular expressions, as well
    as functions for counting characters, words, and sentences, and a function
    for splitting longer texts into separate documents, each with the same number
    of words.
License: MIT + file LICENSE
LazyData: yes
Authors@R: c(person("Lincoln", "Mullen", role = c("aut", "cre"),
        email = "lincoln@lincolnmullen.com",
        comment = c(ORCID = "0000-0001-5103-6917")),
        person("Oliver", "Keyes", role = c("ctb"),
        email = "ironholds@gmail.com"),
        person("Dmitriy", "Selivanov", role = c("ctb"),
        email = "selivanov.dmitriy@gmail.com"),
        person("Jeffrey", "Arnold", role = c("ctb"),
        email = "jeffrey.arnold@gmail.com"),
        person("Kenneth", "Benoit", role = c("ctb"),
        email = "kbenoit@lse.ac.uk"),
        person("Matthew", "Jockers", role = c("ctb"),
        comment = "Jockers stopwords list"))
URL: https://github.com/ropensci/tokenizers
BugReports: https://github.com/ropensci/tokenizers/issues
RoxygenNote: 6.0.1
Depends:
  R (>= 3.1.3)
Imports:
  stringi (>= 1.0.1),
  Rcpp (>= 0.12.3),
  SnowballC (>= 0.5.1)
LinkingTo: Rcpp
Suggests: testthat,
    covr,
    knitr,
    rmarkdown
VignetteBuilder: knitr
